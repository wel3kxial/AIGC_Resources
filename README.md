# AIGC_Resources
Gather AIGC most useful tools, materials, publications and reports

<h2> Foundation Papers </h2>

<table>
  <thead>
    <tr>
      <th>Title</th>
      <th>Model</th>
      <th>Publication Date</th>
      <th>Code</th>
      <th>Organization</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank"><b>Attention Is All You Need</b></a></td>
      <td>Transformer</td>
      <td>Dec 2017</td>
      <td></td>
      <td>Google</td>
    </tr>
    <tr>
      <td><a href="https://arxiv.org/pdf/2108.07258.pdf" target="_blank"><b>Improving Language Understanding by Generative Pre-Training</b></a></td>
      <td>GPT</td>
      <td>Jun 2018</td>
      <td></td>
      <td>OpenAI</td>
    </tr>
    <tr>
      <td><a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank"><b>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</b></a></td>
      <td>Bert</td>
      <td>May 2019</td>
      <td></td>
      <td>Google</td>
    </tr>
    <tr>
      <td><a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank"><b>On the Opportunities and Risks of Foundation Models</b></a></td>
      <td></td>
      <td>Jul 2022</td>
      <td></td>
      <td>Center for Research on Foundation Models (CRFM) & Stanford Institute for Human-Centered Artificial Intelligence (HAI)</td>
    </tr>
    <tr>
      <td><a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" target="_blank"><b>Language Models are Unsupervised Multitask Learners</b></a></td>
      <td>GPT-2</td>
      <td>Dec 2020</td>
      <td><a href="https://github.com/openai/gpt-2" target="_blank">Code</a></td>
      <td>OpenAI</td>
    </tr>
    <tr>
      <td><a href="https://arxiv.org/pdf/2103.00020.pdf" target="_blank"><b>Learning Transferable Visual Models From Natural Language Supervision</b></a></td>
      <td>CLIP</td>
      <td>Feb 2021</td>
      <td><a href="https://github.com/openai/CLIP" target="_blank">Code</a></td>
      <td>OpenAI</td>
    </tr>
    <tr>
      <td><a href="https://arxiv.org/pdf/2107.03374.pdf" target="_blank"><b>Evaluating Large Language Models Trained on Code</b></a></td>
      <td>Codex</td>
      <td>Jul 2021</td>
      <td></td>
      <td>OpenAI</td>
    </tr>
    <tr>
      <td><a href="https://arxiv.org/pdf/2203.07814.pdf" target="_blank"><b>Competition-Level Code Generation with AlphaCode</b></a></td>
      <td>AlphaCode</td>
      <td>Feb 2022</td>
      <td></td>
      <td>DeepMind</td>
    </tr>
     <tr>
      <td><a href="https://arxiv.org/pdf/2302.05543.pdf" target="_blank"><b>Adding Conditional Control to Text-to-Image Diffusion Models</b></a></td>
      <td>ControlNet</td>
      <td>Feb 2022</td>
      <td><a href="https://github.com/lllyasviel/ControlNet" target="_blank">Code</a></td>
      <td>Stanford University</td>
    </tr>
    <tr>
      <td><a href="https://arxiv.org/pdf/2203.13474.pdf" target="_blank"><b>Codegen: an open large language model for code with multi-turn program synthesis</b></a></td>
      <td>CodeGen</td>
      <td>March 2022</td>
      <td><a href="https://github.com/salesforce/CodeGen" target="_blank">Code</a></td>
      <td>Salesforce</td>
    </tr>
  </tbody>
</table>

<h2> Recent Papers </h2>
  <table>
    <tr>
      <th>Title</th>
      <th>Short Name</th>
      <th>Date</th>
      <th>Institution</th>
      <th>Code (if available)</th>
    </tr>
    <tr>
      <td><a href="https://arxiv.org/pdf/2203.02155.pdf" target="_blank"> Training language models to follow instructions with human feedback </a></td>
      <td>Instruct GPT</td>
      <td>March 2022</td>
      <td>OpenAI</td>
      <td></td>
    </tr>
    <tr>
      <td><a href="https://arxiv.org/pdf/2112.10752.pdf" target="_blank"> High-Resolution Image Synthesis with Latent Diffusion Models </a></td>
      <td>Stable Diffusion</td>
      <td>April 2022</td>
      <td>Heidelberg University & Runway</td>
      <td></td>
    </tr>
    <tr>
      <td><a href="https://cdn.openai.com/papers/dall-e-2.pdf" target="_blank"> Hierarchical Text-Conditional Image Generation with CLIP Latents </a></td>
      <td>Dalle 2</td>
      <td>April 2022</td>
      <td>OpenAI</td>
      <td></td>
    </tr>
    <tr>
      <td><a href="https://arxiv.org/pdf/2204.05862.pdf" target="_blank"> Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback </a></td>
      <td>RLHF</td>
      <td>Jun 2022</td>
      <td>Anthropic</td>
      <td></td>
    </tr>
    <tr>
      <td><a href="https://arxiv.org/pdf/2005.14165.pdf" target="_blank"> Language Models are Few-Shot Learners </a></td>
      <td>GPT-3</td>
      <td>Jun 2022</td>
      <td>OpenAI</td>
      <td></td>
    </tr>
    <tr>
  <td><a href="https://arxiv.org/pdf/2112.09332.pdf" target="_blank">WebGPT: Browser-assisted question-answering with human feedback</a></td>
  <td>WebGPT</td>
  <td>Jun 2022</td>
  <td>OpenAI</td>
  <td></td>
</tr>
<tr>
  <td><a href="https://cdn.openai.com/papers/whisper.pdf" target="_blank">Robust Speech Recognition via Large-Scale Weak Supervision</a></td>
  <td>Whisper</td>
  <td>Sep 2022</td>
  <td>OpenAI</td>
  <td><a href="https://github.com/openai/whisper" target="_blank">Code</a></td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/2302.13971.pdf" target="_blank">LLaMA: Open and Efficient Foundation Language Models</a></td>
  <td>LLaMA</td>
  <td>Feb 2023</td>
  <td>Meta</td>
  <td></td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/2303.04671.pdf" target="_blank">Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models</a></td>
  <td>Visual ChatGPT</td>
  <td>March 2023</td>
  <td>Microsoft</td>
  <td><a href="https://github.com/microsoft/visual-chatgpt" target="_blank">Code</a></td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/2303.01469.pdf" target="_blank">Consistency Models</a></td>
  <td></td>
  <td>March 2023</td>
  <td>OpenAI</td>
  <td><a href="https://github.com/openai/consistency_models" target="_blank">Code</a></td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/2302.14045.pdf" target="_blank">Language Is Not All You Need: Aligning Perception with Language Models</a></td>
  <td>Aligning</td>
  <td>March 2023</td>
  <td>Microsoft</td>
  <td></td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/2303.08774.pdf" target="_blank">GPT-4 Technical Report</a></td>
  <td>GPT-4</td>
  <td>March 2023</td>
  <td>OpenAI</td>
  <td></td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/2303.17580.pdf" target="_blank">BloombergGPT: A Large Language Model for Finance</a></td>
  <td>BloombergGPT</td>
  <td>March 2023</td>
  <td>Bloomberg</td>
  <td></td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/2303.17580.pdf" target="_blank">HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face</a></td>
  <td>HuggingGPT</td>
  <td>April 2023</td>
  <td>Microsoft</td>
  <td><a href="https://github.com/microsoft/JARVIS" target="_blank">Code</a></td>
</tr>
<tr>
  <td><a href="https://ai.facebook.com/research/publications/segment-anything/" target="_blank">Segment Anything</a></td>
  <td>SAM</td>
  <td>April 2023</td>
  <td>Meta</td>
  <td><a href="https://github.com/facebookresearch/segment-anything" target="_blank">Code</a></td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/2304.03277.pdf" target="_blank">Instruction Tuning with GPT-4</a></td>
  <td></td>
  <td>April 2023</td>
  <td>Stanford & Google</td>
  <td><a href="https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM" target="_blank">Code</a></td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/2304.03442.pdf" target="_blank">Generative Agents: Interactive Simulacra of Human Behavior</a></td>
  <td></td>
  <td>April 2023</td>
  <td>Microsoft</td>
  <td></td>
</tr>
 <tr>
  <td><a href="https://arxiv.org/pdf/2304.08177.pdf" target="_blank">Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca</a></td>
  <td>Chinese LLaMa</td>
  <td>April 2023</td>
  <td>Microsoft</td>
  <td><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca" target="_blank">Code</a></td>  
</tr>
 <tr>
  <td><a href="https://arxiv.org/pdf/2304.01373.pdf" target="_blank">Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling</a></td>
  <td>Pythia</td>
  <td>April 2023</td>
  <td>MMany</td>
  <td></td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/2304.07193.pdf" target="_blank">DINOv2: Learning Robust Visual Features without Supervision</a></td>
  <td>DINOv2</td>
  <td>April 2023</td>
  <td>Meta</td>
  <td></td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/2305.02463v1.pdf" target="_blank">Shap·E: Generating Conditional 3D Implicit Functions</a></td>
  <td>Shap·E</td>
  <td>May 2023</td>
  <td>OpenAI</td>
  <td><a href="https://github.com/openai/shap-e" target="_blank">Code</a></td>
</tr>
 <tr>
  <td><a href="https://arxiv.org/pdf/2305.06161.pdf" target="_blank">StarCoder: may the source be with you</a></td>
  <td>StarCoder</td>
  <td>May 2023</td>
  <td>Many</td>
  <td><a href="https://github.com/openai/shap-e" target="_blank">Code</a></td>
</tr>
 <tr>
  <td><a href="https://arxiv.org/pdf/2305.08845.pdf" target="_blank">Large Language Models are Zero-Shot Rankers for
Recommender Systems</a></td>
  <td></td>
  <td>May 2023</td>
  <td>Renmin University, Wechat, US San Diego</td>
  <td></td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/2305.05665.pdf" target="_blank">IMAGEBIND: One Embedding Space To Bind Them All</a></td>
  <td>IMAGEBIND</td>
  <td>May 2023</td>
  <td>Meta</td>
  <td></td>
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/2305.10973.pdf" target="_blank">Drag Your GAN: Interactive Point-based Manipulation on the
Generative Image Manifold</a></td>
  <td>DragGAN</td>
  <td>May 2023</td>
  <td>Many</td>
  <td><a href="https://vcai.mpi-inf.mpg.de/projects/DragGAN/" target="_blank">Code</a></td>
</tr>
 <tr>
  <td><a href="https://arxiv.org/pdf/2305.16291.pdf" target="_blank">VOYAGER: An Open-Ended Embodied Agent
with Large Language Models</a></td>
  <td>VOYAGER</td>
  <td>May 2023</td>
  <td>NVIDIA and Many</td>
  <td><a href="https://github.com/MineDojo/Voyager" target="_blank">Code</a></td>
</tr>
 <tr>
  <td><a href="https://arxiv.org/pdf/2305.17126.pdf" target="_blank">Large Language Models as Tool Makers</a></td>
  <td></td>
  <td>May 2023</td>
  <td>Deepmind, Princeton University, Stanford University</td>
  <td><a href="https://github.com/ctlllll/LLM-ToolMaker" target="_blank">Code</a></td>
</tr>
   <tr>
  <td><a href="https://arxiv.org/pdf/2212.10560.pdf" target="_blank">SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions</a></td>
  <td>SELF-INSTRUCT</td>
  <td>May 2023</td>
  <td>Many</td>
  <td><a href="https://github.com/yizhongw/self-instruct" target="_blank">Code</a></td>
</tr>
   <tr>
  <td><a href="https://arxiv.org/pdf/2305.11206.pdf" target="_blank">LIMA: Less Is More for Alignment</a></td>
  <td>LIMA</td>
  <td>May 2023</td>
  <td>Meta, CMU and Many</td>
  <td></td>
</tr>
  <tr>
  <td><a href="https://arxiv.org/pdf/2305.18752.pdf" target="_blank">GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction</a></td>
  <td>GPT4Tools</td>
  <td>May 2023</td>
  <td>Tsinghua University and Many</td>
 <td><a href="https://github.com/StevenGrove/GPT4Tools" target="_blank">Code</a></td>
<tr>
  <tr>
  <td><a href="https://arxiv.org/pdf/2212.10560.pdf" target="_blank">SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions</a></td>
  <td>SELF-INSTRUCT</td>
  <td>May 2023</td>
  <td>Many</td>
 <td><a href="https://github.com/yizhongw/self-instruct" target="_blank">Code</a></td>
<tr>
 <tr>
  <td><a href="https://arxiv.org/pdf/2305.11147.pdf" target="_blank">UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild</a></td>
  <td>UniControl</td>
  <td>May 2023</td>
  <td>Salesforce, Stanford University, Northeastern University</td>
 <td><a href="https://github.com/salesforce/UniControl" target="_blank">Code</a></td>
<tr>
 <tr>
  <td><a href="https://arxiv.org/pdf/2305.14314.pdf" target="_blank">QLORA: Efficient Finetuning of Quantized LLMs</a></td>
  <td>QLORA</td>
  <td>May 2023</td>
  <td>University of Washington</td>
 <td><a href="https://github.com/artidoro/qlora" target="_blank">Code</a></td>
<tr>
 <tr>
  <td><a href="https://arxiv.org/pdf/2305.14387.pdf" target="_blank">AlpacaFarm: A Simulation Framework for
Methods that Learn from Human Feedback</a></td>
  <td>AlpacaFarm</td>
  <td>May 2023</td>
  <td>Stanford University</td>
 <td></td>
<tr>
 <tr>
  <td><a href="https://arxiv.org/pdf/2306.00937.pdf" target="_blank">STEVE-1: A Generative Model for
Text-to-Behavior in Minecraft</a></td>
  <td>STEVE-1</td>
  <td>June 2023</td>
  <td>University of Toronto</td>
  <td></td>
<tr>
  <td><a href="https://arxiv.org/pdf/2306.06070.pdf" target="_blank">MIND2WEB: Towards a Generalist Agent for the Web</a></td>
  <td>MIND2WEB</td>
  <td>June 2023</td>
  <td>Ohio State</td>
   <td><a href="https://osu-nlp-group.github.io/Mind2Web/" target="_blank">Code</a></td> 
</tr>
 <tr>
  <td><a href="https://arxiv.org/pdf/2306.00983.pdf" target="_blank">StyleDrop: Text-to-Image Generation in Any Style</a></td>
  <td>StyleDrop</td>
  <td>June 2023</td>
  <td>Google</td>
   <td><a href="https://styledrop.github.io/" target="_blank">Code</a></td> 
</tr>
 <tr>
  <td><a href="https://arxiv.org/pdf/2306.05284.pdf" target="_blank">Simple and Controllable Music Generation</a></td>
  <td>MusicGen</td>
  <td>June 2023</td>
  <td>Meta</td>
   <td><a href="https://github.com/facebookresearch/audiocraft" target="_blank">Code</a></td> 
</tr>
</tr>
 <tr>
  <td><a href="https://arxiv.org/pdf/2306.02707.pdf" target="_blank">Orca: Progressive Learning from Complex Explanation Traces of GPT-4</a></td>
  <td>Orca</td>
  <td>June 2023</td>
  <td>Microsoft</td>
   <td></td> 
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/2306.08276.pdf" target="_blank">TryOnDiffusion: A Tale of Two UNets</a></td>
  <td>TryOnDiffusion</td>
  <td>June 2023</td>
  <td>University of Washington， Google</td>
   <td></td> 
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/2304.12244.pdf" target="_blank">WizardLM: Empowering Large Language Models to Follow Complex Instructions</a></td>
  <td>WizardLM</td>
  <td>June 2023</td>
  <td>Microsoft, Peking University</td>
  <td><a href="https://github.com/nlpxucan/WizardLM" target="_blank">Code</a></td> 
</tr>
<tr>
  <td><a href="https://research.facebook.com/publications/voicebox-text-guided-multilingual-universal-speech-generation-at-scale/" target="_blank">Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale</a></td>
  <td>Voicebox</td>
  <td>June 2023</td>
  <td>Meta</td>
   <td></td> 
</tr>
<tr>
  <td><a href="https://arxiv.org/pdf/2306.14435.pdf" target="_blank">DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image
Editing</a></td>
  <td>DragDiffusion</td>
  <td>June 2023</td>
  <td>National University of Singapore & ByteDance</td>
   <td></td> 
</tr>
      <tr>
  <td><a href="https://arxiv.org/pdf/2306.11644.pdf" target="_blank">Textbooks Are All You Need</a></td>
  <td>phi-1</td>
  <td>June 2023</td>
  <td>Microsoft</td>
   <td></td> 
</tr>
     <tr>
  <td><a href="https://arxiv.org/pdf/2307.08691.pdf" target="_blank">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a></td>
  <td>FlashAttention-2/td>
  <td>July 2023</td>
  <td>Stanford University</td>
   <td></td> 
</tr>
    <tr>
  <td><a href="https://arxiv.org/pdf/2307.09288.pdf" target="_blank">Llama 2: Open Foundation and Fine-Tuned Chat Models</a></td>
  <td>Llama 2</td>
  <td>July 2023</td>
  <td>Meta</td>
   <td></td> 
</tr>
  </table>


<h2> Important Reports </h2>

<table>
  <tr>
    <th>Report</th>
    <th>Link</th>
    <th>Date</th>
    <th>Institution</th>
  </tr>
  <tr>
    <td>Stanford AI index Report 2023</td>
    <td><a href="https://aiindex.stanford.edu/report/" target="_blank">Link</a></td>
    <td></td>
    <td>Stanford</td>
  </tr>
  <tr>
    <td>Sparks of Artificial General Intelligence: Early experiments with GPT-4</td>
    <td><a href="https://arxiv.org/pdf/2303.12712.pdf" target="_blank">Link</a></td>
    <td></td>
    <td>Microsoft</td>
  </tr>
  <tr>
    <td>A Survey of Large Language Models</td>
    <td><a href="https://arxiv.org/pdf/2303.18223.pdf" target="_blank">Link</a></td>
    <td>April 2023</td>
    <td>Renmin University, China & University of Montreal, Canada</td>
  </tr>
  <tr>
    <td>Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond</td>
    <td><a href="https://arxiv.org/pdf/2304.13712v2.pdf" target="_blank">Link</a></td>
    <td></td>
    <td>Amazon & many others</td>
  </tr>
  <tr>
    <td>A Cookbook of Self-Supervised Learning</td>
    <td><a href="https://arxiv.org/pdf/2304.12210.pdf" target="_blank">Link</a></td>
    <td></td>
    <td>Meta & many others</td>
  </tr>
    <tr>
    <td>Let’s Verify Step by Step</td>
    <td><a href="https://cdn.openai.com/improving-mathematical-reasoning-with-process-supervision/Lets_Verify_Step_by_Step.pdf" target="_blank">Link</a>    </td>
     <td>May 2023</td>
    <td>OpenAI</td>
  </tr>
   <tr>
    <td>A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering</td>
    <td><a href="https://arxiv.org/pdf/2306.06211.pdf" target="_blank">Link</a></td>
    <td>May 2023</td>
    <td>Kyung Hee University and many</td>
  </tr>
  <tr>
    <td>A Comprehensive Survey on Segment Anything Model for Vision and Beyond</td>
    <td><a href="https://arxiv.org/pdf/2305.08196.pdf" target="_blank">Link</a></td>
    <td>May 2023</td>
    <td>Hong Kong University of Science and Technology and many</td>
  </tr>
  <tr>
    <td>On the Design Fundamentals of Diffusion Models: A Survey</td>
    <td><a href="https://arxiv.org/pdf/2306.04542.pdf" target="_blank">Link</a></td>
    <td>June 2023</td>
    <td>Durham University</td>
  </tr>
    <tr>
    <td>Open LLM Leaderboard</td>
    <td><a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" target="_blank">Link</a></td>
    <td>Update in real time</td>
    <td>Huggingface</td>
  </tr>
    <tr>
    <td>A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering</td>
    <td><a href="https://arxiv.org/pdf/2306.06211.pdf" target="_blank">Link</a></td>
    <td>May 2023</td>
    <td>JKyung Hee University and many</td>
  </tr>
   <tr>
    <td>Large Multimodal Models: Notes on CVPR 2023 Tutoria/td>
    <td><a href="https://arxiv.org/pdf/2306.14895.pdf" target="_blank">Link</a></td>
    <td>June 2023</td>
    <td>Microsoft</td>
  </tr>
  <tr>
    <td>Recent Advancements in End-to-End Autonomous Driving using Deep Learning: A Survey</td>
    <td><a href="https://arxiv.org/pdf/2307.04370.pdf" target="_blank">Link</a></td>
    <td>July 2023</td>
    <td>IIT</td>
  </tr>
     <tr>
    <td>Challenges and Applications of Large Language Models </td>
    <td><a href="https://arxiv.org/pdf/2307.10169.pdf" target="_blank">Link</a></td>
    <td>July 2023</td>
    <td>UCL and many</td>
  </tr>
</table>



<h2> Important Projects</h2>

<b>  <a href="https://www.midjourney.com/" target="_blank">  MidJourney </a>    </b>  

<b>  <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html" target="_blank">  Alpaca  </a>    </b>    <a href="https://github.com/tatsu-lab/stanford_alpaca" target="_blank"> Open Source Code </a>  Stanford    March 2023

<b>  <a href="https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html" target="_blank">  Dolly  </a>    </b>    <a href="https://github.com/databrickslabs/dolly" target="_blank"> Open Source Code </a>  Databricks     March 2023   Note: OK to use commercially 

<b>  <a href="https://vicuna.lmsys.org/" target="_blank">  Vicuna  </a>    </b>    <a href="https://github.com/lm-sys/FastChat" target="_blank"> Open Source Code </a>   UC Berkeley, CMU, Stanford, and UC San Diego     March 2023
 
<b>  <a href="https://www.chatpdf.com/" target="_blank">  ChatPDF  </a>  </b>   March 2023

<b>  <a href="https://bard.google.com/" target="_blank">  Bard  </a>    </b>   Google   March 2023

<b>  <a href="https://github.com/hwchase17/langchain" target="_blank">  Langchain </a>    </b>   Community Effort   March 2023

<b>  <a href="https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/" target="_blank">  Microsoft 365 Copilot  </a>    </b>   Microsoft   March 2023

<b>  <a href="https://github.com/Torantulino/Auto-GPT" target="_blank">  AutoGPT </a>    </b>   Community Effort  April 2023

<b>  <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything" target="_blank">  Grounded SAM </a>    </b>   IDEA  April 2023

<b>  <a href="https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat" target="_blank">  DeepSpeed Chat </a>    </b>   Microsoft  April 2023

<b>  <a href="https://github.com/reworkd/AgentGPT" target="_blank">  AgentGPT </a>    </b>   Community Effort  April 2023

<b>  <a href="https://minigpt-4.github.io/" target="_blank">  MiniGPT </a>    </b>   King Abdullah University of Science and Technology  April 2023

<b>  <a href="https://github.com/deep-floyd/IF" target="_blank">  DeepFloyd IF </a>    </b>  Stability.ai April 2023

<b>  <a href="https://github.com/openlm-research/open_llama" target="_blank">  Open Llama </a>    </b>  Berkeley  May 2023

<b>  <a href="https://github.com/svc-develop-team/so-vits-svc" target="_blank">  SoftVC VITS Singing Voice Conversion </a>    </b>  Community  May 2023

<b>  <a href="https://falconllm.tii.ae/" target="_blank">  Falcon </a>    </b>  Tii  May 2023

<b>  <a href="https://github.com/thunlp/UltraChat" target="_blank">  UltraLM </a>    </b>  Tsinghua University  June 2023

<b>  <a href="https://github.com/PKU-YuanGroup/ChatLaw" target="_blank">  ChatLaw </a>    </b>  Peking University  June 2023

<b>  <a href="https://github.com/OptimalScale/LMFlow" target="_blank">  LMFlow </a>    </b>  HK University of Science and Technology  June 2023

<h2> AIGC Courses </h2>

<b>  <a href="https://www.cs.princeton.edu/courses/archive/fall22/cos597G/" target="_blank">  COS597G Understanding Large Language Models  </a>    </b>   Princeton 2022

<b>  <a href="https://stanford-cs324.github.io/winter2022/" target="_blank">  CS324 Large Language Models  </a>    </b>   Stanford  2023

<b>  <a href="https://www.deeplearning.ai/short-courses/" target="_blank">  ChatGPT, LangChain and DS Courses  </a>    </b>   Deeplearning.ai   Jun  2023

<b>  <a href="https://arxiv.org/pdf/2306.14895.pdf" target="_blank"> Large Multimodal Models: Notes on CVPR 2023 Tutorial  </a>    </b>   Microsoft   Jun  2023



<h2> Very Useful Source Code </h2>

<b>  <a href="https://github.com/openai/openai-cookbook/" target="_blank">  OpenAI Cookbook  </a>    </b>  
<b>  <a href="https://gpt-index.readthedocs.io/en/latest/" target="_blank">  Llama Index </a>    </b>  
<b>  <a href="https://github.com/imartinez/privateGPT" target="_blank">  PrivateGPT</a>    </b>  
<b>  <a href="https://github.com/ggerganov/llama.cpp" target="_blank">  Llama.cpp </a>    </b> 

<h2> Main LLM Development Tips,   Updated June 21, 2023 </h2>
<p> 1. Data is still king - LLMs are great but if you don't have quality clean data you won’t go far.</p>
<p>2. Smaller models can be just as good as larger general models at specific tasks. And cheaper!</p>
<p>3. Fine-tuning is becoming cheaper.</p>
<p>4. Evaluation of LLMs is very hard - feels very subjective still.</p>
<p>5. Managed APIs are expensive.</p>
<p>6. "Traditional" ML isn't going anywhere.</p>
<p>7. Memory matters - for both serving and training.</p>
<p>8. Information retrieval w/ vector databases is becoming standard pattern.</p>
<p>9. Start w/ prompt engineering and push that to its limits before fine-tuning w/ smaller models.</p>
<p>10. Use agents/chains only when necessary. They are unruly.</p>
<p>11. Latency is critical for a good user experience.</p>
<p>12. Privacy is critical.  </p>

<h2> Main LLM Development Tips,   Updated July 16, 2023 </h2>
<p>1. Prompt vs fine-tuning: The reliability of prompt engineering is still not enough, it is sensitive to certain prompts, and supervised fine-tuning (sft) remains a stable and efficient method.
<p>2. The ability of open-source models and the gap with GPT-4 still lies in the complexity of the base model. Although the answer styles can be similar, the professional content and reasoning capabilities differ greatly. A better base model is still the key. Llama2 is about to be released and may be commercialized. The evaluation of the abilities of pretrained models is mainly based on a large set of tasks.
<p>3. sft vs ppo: training and using ppo is still difficult, ppo can indeed improve results. There is a lot of academic research, but there are not many commercial applications yet. sft, when combined with good data, can replace ppo in most cases. Fine-tuning indeed improves tool usage and comprehensive summary response capabilities for specific domains.
<p>4. Key points for sft data: diversity, not only the content and perspective of the problems should be diverse, but also the style of questioning. In terms of answers, not only the accuracy and truthfulness of the content are important, but also the format and style should be as expected (for example, clear and organized).
<p>5. Regarding the link between pretraining and fine-tuning, when there is less data in the fine-tuning dataset, you can consider adding some pretraining datasets to increase stability. There are also many stages added between pretraining and fine-tuning, namely continuous pretraining. At this stage, you mainly train the domain you are focusing on, which will increase the recognition of this domain, such as training a dedicated coder, doing data and code mixed data training in pretraining, and doing coding code data training in continuous pretraining.
<p>6. OpenLlama pre-training computing power: hundreds of GPUs, 1-2 months.
<p>7. LangChain _ Vec DB is actually retrieval & tool use.
<p>8. Fine-tuning vs Vec DB, fine-tuning is more about understanding large amounts of information, Vec DB is more about specific data details. There is no conflict between the two. Fine-tuning can consider turning the data retrieved by Vec DB into fine-tuning example data.
<p>9. The optimization of the combination of fine-tuning and Vec DB, by generating related question keywords or sql through llm to retrieve relevant data from the database or knowledge base, then let llm summarize, and then put the entire process into the fine-tuning training dataset, will greatly enhance the effect. The problem of keyword matching can be solved by collecting a dataset of a few thousand examples, which will not be too difficult to teach llm this ability. Keyword matching is crucial for improving the tool use capability of fine-tuning. Pretrained models are not tools, they need to collect a large amount of data for fine-tuning to know that this task needs to search, and that task needs to use a mathematical model.
<p>10. When keyword matching encounters more complex structured problems, llm can generate sql or python to solve it.
<p>11. Many people are doing natural language processing to do data science, that is, llm generates sql to query relational databases, and this scenario can also be fine-tuned.
<p>12. The reasoning, decision-making, and error correction of llm still depend on the base model capability. Generally, there is still a part of such data in the pretraining data of the model, but it has not been specialized. If you are biased towards traditional decision-making problems, you can strengthen this ability by specifically doing rewards and labels in fine-tuning.
<p>13.Multimodal is a key direction. The open-source model community is starting to find that multimodal is not that difficult. The current mainstream method is to add multimodal during fine-tuning (not during pretraining). If you have computing power and data, fine-tuning with multimodal is better, it does not lose information. If it is a picture-to-text method, information will be lost. Diffusion models are still mainstream for generating images, because they have fewer computations and parameters, but autoregressive models have also been proven to have good image generation capabilities (palm e: first tokenize the picture and then use llm to generate the picture), but if the hardware improves further, there may be multimodal generation models that use autoregressive models, the model is simpler.
<p>14. The long-tail problem (medical, autonomous driving and other critical scenarios) can be improved by collecting data similar to long-tail through simulated scenarios, usually data from accidents. For non-real-time scenarios like medical, it may still require human oversight.

